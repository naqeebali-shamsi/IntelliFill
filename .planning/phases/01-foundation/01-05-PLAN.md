---
phase: 01-foundation
plan: 05
type: execute
---

<objective>
Harden the smart-profile extraction pipeline to address P0 critical issues from expert review.

Purpose: Fix production-blocking issues that cause silent data corruption, unbounded API costs, and user confusion.
Output: Resilient extraction pipeline with proper error handling, rate limiting, and validated JSON responses.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-03-SUMMARY.md

**Files to modify:**
@quikadmin/src/api/smart-profile.routes.ts
@quikadmin/src/multiagent/agents/extractorAgent.ts

**Expert Review Context:**
P0 issues identified by 5 expert agents (Agentic AI, LLM Engineering, Document AI, MLOps, AI UX):

1. Silent error swallowing - batch extraction continues on failure, corrupting data integrity
2. No request timeout on Gemini - can block indefinitely
3. No rate limiting on Gemini - unlimited API calls = cost exposure
4. No JSON schema enforcement - ~5% silent failures from malformed LLM responses
   </context>

<tasks>

<task type="auto">
  <name>Task 1: Fix silent error swallowing in batch extraction</name>
  <files>quikadmin/src/api/smart-profile.routes.ts</files>
  <action>
Replace "continue processing" anti-pattern with proper file-level error tracking:

1. Add `errors` array to ExtractBatchResponse:

   ```typescript
   interface FileError {
     fileId: string;
     fileName: string;
     error: string;
     stage: 'ocr' | 'extraction' | 'merge';
   }

   interface ExtractBatchResponse {
     success: boolean;
     profileData: Record<string, unknown>;
     fieldSources: Record<string, FieldSource>;
     lowConfidenceFields: LowConfidenceField[];
     errors: FileError[]; // NEW: track per-file errors
     processingTime: number;
     documentsProcessed: number;
     successfulDocuments: number; // NEW: distinguish from errors
     totalFieldsExtracted: number;
   }
   ```

2. In extractBatchHandler, change catch block from silent continue to error tracking:
   - Capture error with fileId, fileName, error message, and stage
   - Push to errors array
   - Continue processing other files (maintain partial success)
   - Set success: true ONLY if errors.length === 0

3. Log errors at WARN level with structured data for debugging.

AVOID: Throwing on first error (breaks partial extraction).
AVOID: Returning success: true when errors exist.
</action>
<verify>

1. Upload a valid document and an invalid document together
2. Response should include profileData from valid doc AND errors array with invalid doc error
3. success should be false when any errors exist
   </verify>
   <done>

- ExtractBatchResponse includes errors array with per-file tracking
- Partial extraction works (valid files processed despite invalid ones)
- success boolean accurately reflects error state
- Errors logged at WARN level with fileId for debugging
  </done>
  </task>

<task type="auto">
  <name>Task 2: Add request timeout and rate limiting for Gemini API</name>
  <files>quikadmin/src/multiagent/agents/extractorAgent.ts</files>
  <action>
Add timeout and rate limiting to Gemini API calls:

1. Add timeout wrapper for Gemini calls:

   ```typescript
   const GEMINI_TIMEOUT_MS = 30000; // 30 seconds max per extraction

   async function withTimeout<T>(promise: Promise<T>, ms: number, operation: string): Promise<T> {
     const timeout = new Promise<never>((_, reject) =>
       setTimeout(() => reject(new Error(`${operation} timed out after ${ms}ms`)), ms)
     );
     return Promise.race([promise, timeout]);
   }
   ```

2. Apply timeout to generateContent calls:

   ```typescript
   const result = await withTimeout(
     model.generateContent({ contents: [{ role: 'user', parts }] }),
     GEMINI_TIMEOUT_MS,
     'Gemini extraction'
   );
   ```

3. Add simple rate limiting with p-limit (already in deps) or manual semaphore:

   ```typescript
   import pLimit from 'p-limit';

   // Max 5 concurrent Gemini calls to prevent cost spikes
   const geminiLimit = pLimit(5);

   // In extraction function:
   const result = await geminiLimit(() =>
     withTimeout(model.generateContent(...), GEMINI_TIMEOUT_MS, 'Gemini extraction')
   );
   ```

4. Log rate limit events and timeouts at WARN level.

AVOID: No limit at all (cost exposure).
AVOID: Limit of 1 (too slow for batch).
</action>
<verify>

1. `grep -r "withTimeout" quikadmin/src/multiagent/agents/extractorAgent.ts` shows timeout wrapper
2. `grep -r "pLimit\|geminiLimit" quikadmin/src/multiagent/agents/extractorAgent.ts` shows rate limiting
3. Unit test with mocked slow response confirms timeout works
   </verify>
   <done>

- 30-second timeout on all Gemini API calls
- Max 5 concurrent Gemini calls enforced
- Timeout errors surface as extraction errors (not silent hangs)
- Rate limit events logged for monitoring
  </done>
  </task>

<task type="auto">
  <name>Task 3: Add JSON schema validation for Gemini responses</name>
  <files>quikadmin/src/multiagent/agents/extractorAgent.ts</files>
  <action>
Add Zod schema validation to catch malformed LLM responses:

1. Define extraction response schema:

   ```typescript
   import { z } from 'zod';

   const ExtractedFieldSchema = z.object({
     value: z.union([z.string(), z.number(), z.boolean(), z.null()]),
     confidence: z.number().min(0).max(100).optional(),
     rawText: z.string().optional(),
   });

   const GeminiExtractionSchema = z.record(z.string(), ExtractedFieldSchema);
   ```

2. Wrap JSON.parse in validation:

   ````typescript
   function parseGeminiResponse(text: string): GeminiExtractionResponse {
     // Extract JSON from markdown code blocks if present
     const jsonMatch = text.match(/```(?:json)?\s*([\s\S]*?)```/);
     const jsonStr = jsonMatch ? jsonMatch[1].trim() : text.trim();

     const parsed = JSON.parse(jsonStr);
     const validated = GeminiExtractionSchema.parse(parsed);
     return validated;
   }
   ````

3. Catch ZodError separately from JSON.parse errors:
   - JSON.parse error → log as "Malformed JSON response"
   - ZodError → log as "Schema validation failed" with specific field errors

4. On validation failure, return empty fields with error logged (don't throw).

AVOID: Trusting raw LLM output without validation.
AVOID: Silent failure on schema mismatch.
</action>
<verify>

1. `grep -r "GeminiExtractionSchema\|ExtractedFieldSchema" quikadmin/src/multiagent/agents/extractorAgent.ts` shows schemas
2. `grep -r "ZodError" quikadmin/src/multiagent/agents/extractorAgent.ts` shows error handling
3. Test with malformed response: should log error, return empty fields, not crash
   </verify>
   <done>

- Zod schema validates all Gemini extraction responses
- Malformed JSON and schema mismatches logged with details
- Invalid responses return empty fields (graceful degradation)
- ~5% silent failure rate reduced to ~0% with proper error surfacing
  </done>
  </task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build` in quikadmin succeeds without errors
- [ ] Upload valid+invalid docs → response includes both profileData AND errors array
- [ ] success: false when any file errors exist
- [ ] Grep confirms timeout wrapper and rate limiting in extractorAgent.ts
- [ ] Grep confirms Zod schema validation in extractorAgent.ts
</verification>

<success_criteria>

- All 3 tasks completed and committed
- Error handling captures per-file failures without stopping batch
- 30-second timeout prevents indefinite hangs
- Rate limiting prevents cost spikes (max 5 concurrent)
- Schema validation catches malformed LLM responses
- Build passes, no TypeScript errors
  </success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-05-SUMMARY.md`
</output>
